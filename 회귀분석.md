# 회귀분석(Linear Regression)

기계학습 - 지도학습 - 회귀분석

## 단순선형회귀분석(Simple Linear Regression)

- 회귀분석이란?

대학 운동부 학생들의 신체검사 자료, 신입생 A가 들어왔다.(키는 175cm이다) 예상 몸무게는 얼마인가?   
-> 추세선(Regression)을 따라 175cm의 몸무게를 보았을 떄 65kg인 것을 예측할 수 있다.

## 회귀분석법

당신은 엘리스의 데이터 사이언티스트다.   

- 데이터: 광고 분석과 판매량
- 목표: FB 광고에 얼마를 투자하면… 상품이 얼마나 팔릴까?
- 방법: 데이터를 가장 잘 설명하는 어떤 선을 하나 찾는다.

### 변수 표기
- N : 데이터의 개수
- X : Input; 데이터/Feature “광고료”
- Y : Output; 해답/응답 “판매량”
- (x(i), y(i)): i번째 데이터

`판매량 (천개) : Y축`   
`광고료 (만원) : X축`

### 문제 정의

- 데이터: N개의 FB 광고 예산(X)과 판매량(Y)
- 목표: 광고에 얼마를 투자했을 때 얼마나 팔릴까?

`광고 예산 > 학습된 모델 > 판매량` : X -> Y

- 가정: TV 광고 예산과 판매량은 선형적 관계를 가진다 `Y ~ B0X + B1` B0 = a(기울기), B1 = b(절편)
- 문제: 어떤 β0, β1이 좋은 것인가?

#### 기울기와 절편 실습

```
# 실습에 필요한 패키지입니다. 수정하지 마세요.
import elice_utils
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
eu = elice_utils.EliceUtils()

# 실습에 필요한 데이터입니다. 수정하지마세요. 
X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]

'''
beta_0과 beta_1 을 변경하면서 그래프에 표시되는 선을 확인해 봅니다.
기울기와 절편의 의미를 이해합니다.
'''

# 이 선을 자동으로 찾는 것이 회귀 분석이다.
beta_0 = 0.67   # beta_0에 저장된 기울기 값을 조정해보세요. 
beta_1 = 1 # beta_1에 저장된 절편 값을 조정해보세요.

plt.scatter(X, Y) # (x, y) 점을 그립니다.
plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.

plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.

# 엘리스에 이미지를 표시합니다.
plt.savefig("test.png")
eu.send_image("test.png")
```

### 모델의 학습 목표

- 아이디어: 완벽한 예측은 불가능하다. 각 데이터 (x(i), y(i)) 의 실제 값과 모델이 예측하는 값을 최소한으로 하자!

- i번째 데이터(x(i), y(i)) 에 대해 :
    - 실제 값 : y(i)
    - 예측 값 : B0x(i) + B1
    - 차이 : y(i) - (B0x(i) + B1)
    - 전체 모델의 차이: sum(y(i) - (B0x(i) + B1))
        - 반례 : 두 점을 예측한 값의 오차의 합이 0이 될 수 있다. (양수와 음수의 차이) 나쁜 선을 그었음에도 좋다고 판단될 수 있다.
            - 그래서 오차의 제곱, 오차의 평균등의 방법을 쓰는 것이다.
            - sum((y(i) - (B0x(i) + B1))^2)

### 문제 재정의

- 전체 모델의 차이 : sum((y(i) - (B0x(i) + B1))^2) : “Loss function” = L(B0, B1)
    - 이 차이를 최소로 하는 β0, β1 을 구하자.
    - argmin((y(i) - (B0x(i) + B1))^2)


#### Loss function 실제로 구현해보기

```
import elice_utils
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
eu = elice_utils.EliceUtils()

def loss(x, y, beta_0, beta_1):
    '''
    x, y, beta_0, beta_1 을 이용해 loss값을 계산한 뒤 리턴합니다.
    '''

    N = len(x) # 10
    # 수식을 파이썬 코드로 변환하기
    # 파이썬 array 이용한 방법
    '''
    total_loss = 0 # i가 n일 때 loss값을 더하기 위한 값
    for i in range(N) : # 수식 sum을 변환
        y_i = y[i] # 실제 y(i)
        x_i = x[i] # 실제 x(i)
        y_predicted = beta_0 * x_i + beta_1
        
        # 실제값과 예측값의 오차의 제곱
        diff = (y_i - y_predicted) ** 2
        total_loss += diff
        
    return total_loss
    '''
    
    # Numpy array를 이용해 간단하게 만드는 방법
    x = np.array(x) # 넘파이 형식으로 변경
    y = np.array(y)
    
    # 이번엔 x가 스칼라값이 아니고 numpy의 array이다.(벡터의 모든 자원에 스칼라곱)
    y_predicted = beta_0 * x + beta_1
    
    total_loss = np.sum((y - y_predicted) ** 2)
    
    return total_loss
    

X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]

beta_0 = 1 # 기울기
beta_1 = 0.5 # 절편

print("Loss: %f" % loss(X, Y, beta_0, beta_1))

plt.scatter(X, Y) # (x, y) 점을 그립니다.
plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.

plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.
plt.savefig("test.png") # 저장 후 엘리스에 이미지를 표시합니다.
eu.send_image("test.png")
```

#### 산 정상 오르기

산 정상이 되는 지점을 찾고 싶다. 아무 곳에서나 시작했을 때, **가장 정상을 빠르게** 찾아가는 방법은? (B0, B1의 최적값을 찾기)   

- 가정
    - 정상의 위치는 알 수 없다.
    - 현재 나의 위치와 높이를 알 수 있다.
    - 내 위치에서 일정 수준 이동할 수 있다.

- 방법
    - 현재 위치에서 가장 경사가 높은 쪽  찾는다.
    - 오르막 방향으로 일정 수준 이동한다.
    - 더 이상 높이의 변화가 없을 때까지 반복!

#### 거꾸로 된 산을 내려가기 : 데이터를 가장 잘 설명하는 β0, β1을 구하자
= 예측 값과 실제 값의 차이를 최소로 만드는 값을 구하자   
= Loss function을 최소로 만드는 β0, β1을 구하자   
= L(B0, B1) = sum((y(i) - (B0x(i) + B1))^2)   

### Scikit-learn을 이용한 회귀분석

```
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LinearRegression

import elice_utils
eu = elice_utils.EliceUtils()

def loss(x, y, beta_0, beta_1):
    N = len(x)
    
    '''
    이전 실습에서 구현한 loss function을 여기에 붙여넣습니다.
    '''
    
    x = np.array(x)
    y = np.array(y)
    total_loss = np.sum((y - (beta_0 * x + beta_1))**2)
    
    return total_loss
    
X = [8.70153760, 3.90825773, 1.89362433, 3.28730045, 7.39333004, 2.98984649, 2.25757240, 9.84450732, 9.94589513, 5.48321616]
Y = [5.64413093, 3.75876583, 3.87233310, 4.40990425, 6.43845020, 4.02827829, 2.26105955, 7.15768995, 6.29097441, 5.19692852]

'''
sklearn에서는 벡터와 행렬을 나타내는 방법으로 numpy 배열을 표준으로 사용하고 있습니다. 따라서 X와 Y를 각각 np.array로 변환해야 합니다.

하나의 속성(feature)에 여러가지 값(sample)을 가지는 경우, reshape(-1, 1)을 적용하여 열벡터로 만들어야 합니다. X는 하나의 종속변수 Y에 대한 여러 값을 가지므로 reshape(-1, 1)을 적용합니다.

reshape(-1, 1)은 reshape(10, 1)과 같은 기능을 합니다. 마치 ‘a[-1]은 a라는 리스트의 마지막 원소다’와 비슷한 맥락입니다.
'''

# np.array(X).reshape(-1, 1) 명령어를 이용해 길이 10인 1차원 리스트 X 를 10×1 형태의 np.array로 변경하세요.
train_X = np.array(X).reshape(-1, 1) # 2차원 리스트의 np.array로 바뀐다.
# 종속변수 Y는 독립변수 X에 대한 값이므로 reshape(-1, 1)을 할 필요가 없습니다. 리스트 Y를 np.array 형식으로 변경하세요.
train_Y = np.array(Y)

'''
여기에서 모델을 트레이닝합니다.
'''
lrmodel = LinearRegression() # Scikit-learn의 LinearRegression 모듈 사용
lrmodel.fit(train_X, train_Y)

'''
loss가 최소가 되는 직선의 기울기와 절편을 계산함
'''
beta_0 = lrmodel.coef_[0]   # lrmodel로 구한 직선의 기울기, coefficient의 약자, 단순 회귀 분석을 사용하고 있으므로 변수가 단 하나밖에 없어서 [0]번째가 기울기가 된다.
beta_1 = lrmodel.intercept_ # lrmodel로 구한 직선의 y절편

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("Loss: %f" % loss(X, Y, beta_0, beta_1))

plt.scatter(X, Y) # (x, y) 점을 그립니다.
plt.plot([0, 10], [beta_1, 10 * beta_0 + beta_1], c='r') # y = beta_0 * x + beta_1 에 해당하는 선을 그립니다.

plt.xlim(0, 10) # 그래프의 X축을 설정합니다.
plt.ylim(0, 10) # 그래프의 Y축을 설정합니다.
plt.savefig("test.png") # 저장 후 엘리스에 이미지를 표시합니다.
eu.send_image("test.png")
```

# 다중회귀분석(Multiple Linear Regression)

- 다중 회귀 분석(Multiple Linear Regression)은 데이터의 여러 변수(features, independent variables) X를 이용해 결과 (response variable) Y를 예측하는 모델

엘리스에서 FB광고뿐만 아니라 TV 및 신문 광고도 하기로 결정했다.   
이제 여러분은 각 매체가 얼마나 효율적인지 알아내야 한다.   

- 데이터 가정   
FB에 44.5만원, TV에 39.3만원, 신문에 45.1만원을 집행했을 때 10,400 건의 판매를 기록했다.

- 문제 : FB에 30만원, TV에 100만원, 신문에 50만원의 광고비를 집행했을 때 예상 판매량은 얼마인가?
    - 분석을 통해 어떤 매체가 얼마나 효율적인지 알아내보자.

- Notation
    - N : 데이터의 개수, FB / TV / 신문 / 판매량 (ROW의 수)
    - X : “Input” 데이터 / Feature (광고료) / 입력 데이터의 열(벡터)
        - X1: FB 광고료
        - X2: TV 광고료
        - X3: 신문 광고료
    - Y : “Output” 해답/응답 (판매량) / 해답 데이토의 열(스칼라)
    - (x1(i), x2(i), x3(i), y(i)) : i번째 데이터

## 문제 정의

- 데이터 : N개의 FB, TV, 신문 광고 예산과 판매량   
- 목표 : FB, TV, 신문에 각각 얼마씩을 투자했을 때 얼마나 팔릴까?
- 가설 : 학습 알고리즘이 주어진 데이터를 학습
    - `광고 예산 > 학습되 모델 > 판매량`
- 가정 : 판매량은 FB, TV, 신문 광고료와 선형적 관계
    - Y = B0x1 + B1x2 + B2x3 + B3
    - B3 : 광고를 하지 않았을 때 기본적인 판매량
    - B0, B1, B2는 각 광고 매체별 기울기(가중치)

- 단순선형회귀분석과 동일 : 푸는 방법도 동일      
완벽한 예측은 불가능하다. 각 데이터 (x1(i), x2(i), x3(i), y(i)) 의 실제 값과 모델이 예측하는 값을 최소한으로 하자!   

## 수학적으로 다시 쓰기
- 실제 값: y(i)
- 모델이 B0x1(i) + B1x2(i) + B2x3(i) + B3
- 차이의 제곱 : (y(i) - (B0x1(i) + B1x2(i) + B2x3(i) + B3))^2
- 차이의 제곱의 합 : sum((y(i) - (B0x1(i) + B1x2(i) + B2x3(i) + B3))^2) = Loss function
    - 이 차이를 최소로 하는 β0, β1, β2, β3 을 구하자.

### 다중회귀분석 실습(FB, TV, 신문에 각각 얼마씩을 투자했을 때 얼마나 팔릴까?)

```
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score

'''
./data/Advertising.csv 에서 데이터를 읽어, X와 Y를 만듭니다.

X는 (200, 3) 의 shape을 가진 2차원 np.array,
Y는 (200,) 의 shape을 가진 1차원 np.array여야 합니다.

X는 FB, TV, Newspaper column 에 해당하는 데이터를 저장해야 합니다.
Y는 Sales column 에 해당하는 데이터를 저장해야 합니다.
'''

import csv
csvreader = csv.reader(open("./data/Advertising.csv"))

x = []
y = []

# # column 명인 첫 행(header)을 무시, csv.reader에서 header의 존재를 명시하는 방법도 있다.
next(csvreader) 

for line in csvreader :
    # 인덱스인 첫번째 열은 무시, 각 데이터포인트에 적합한 값들을 넣어준다.
    x_i = [ float(line[1]), float(line[2]), float(line[3]) ] # i번째 데이터의 FB, TV, News 광고료
    y_i = float(line[4]) # i번째 데이터의 해답
    x.append(x_i)
    y.append(y_i)

# 리스트 값을 np.array 형식으로 변환한다.(훈련 데이터)
X = np.array(x)
Y = np.array(y)

lrmodel = LinearRegression()
lrmodel.fit(X, Y)

beta_0 = lrmodel.coef_[0] # 0번째 변수에 대한 계수(기울기) (페이스북)
beta_1 = lrmodel.coef_[1] # 1번째 변수에 대한 계수(기울기) (TV)
beta_2 = lrmodel.coef_[2] # 2번째 변수에 대한 계수(기울기) (신문)
beta_3 = lrmodel.intercept_ # y절편 (기본 판매량)

print("beta_0: %f" % beta_0)
print("beta_1: %f" % beta_1)
print("beta_2: %f" % beta_2)
print("beta_3: %f" % beta_3)

def expected_sales(fb, tv, newspaper, beta_0, beta_1, beta_2, beta_3):
    '''
    FB에 fb만큼, TV에 tv만큼, Newspaper에 newspaper 만큼의 광고비를 사용했고,
    트레이닝된 모델의 weight 들이 beta_0, beta_1, beta_2, beta_3 일 때
    예상되는 Sales 의 양을 출력합니다.
    '''
    
    sales = beta_0 * fb + beta_1 * tv + beta_2 * newspaper + beta_3
    
    return sales

print("예상 판매량: %f" % expected_sales(10, 12, 3, beta_0, beta_1, beta_2, beta_3))
```

# 다항식 회귀분석(Polynomial Linear Regression)

- 단순한 선형회귀법은 데이터를 잘 설명하지 못한다. 조금 더 데이터에 맞게 모델을 학습시킬 수 없을까?
    - 현실의 데이터는 선형의 데이터가 별로 없다. 좀 더 정확한 함수가 필요하다.

- 문제 : 판매량과 광고비의 관계를 2차식으로 표현해 보자.
    - `Y = B0X^2 + B1X + B2`
    - 선형관계가 아닌데 어떻게 문제를 풀지?
        - X의 제곱 값을 새로 만들기

```
1. X1 = X^2
2. X2 = X 로 치환하면…
3. Y = B0X1 + B1X2 + B2 로
다중회귀분석과 동일해진다
```

## 다항식 회귀분석 실습(변수를 다양하게 조합하며 MSE를 1 미만으로 만들기)

```
import numpy as np
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import train_test_split

'''
./data/Advertising.csv 에서 데이터를 읽어, X와 Y를 만듭니다.

X는 (200, 3) 의 shape을 가진 2차원 np.array,
Y는 (200,) 의 shape을 가진 1차원 np.array여야 합니다.

X는 FB, TV, Newspaper column 에 해당하는 데이터를 저장해야 합니다.
Y는 Sales column 에 해당하는 데이터를 저장해야 합니다.
'''

import csv
csvreader = csv.reader(open("./data/Advertising.csv"))

x = []
y = []

# # column 명인 첫 행(header)을 무시, csv.reader에서 header의 존재를 명시하는 방법도 있다.
next(csvreader) 

for line in csvreader :
    # 인덱스인 첫번째 열은 무시, 각 데이터포인트에 적합한 값들을 넣어준다.
    x_i = [ float(line[1]), float(line[2]), float(line[3]) ] # i번째 데이터의 FB, TV, News 광고료
    y_i = float(line[4]) # i번째 데이터의 해답
    x.append(x_i)
    y.append(y_i)

# 리스트 값을 np.array 형식으로 변환한다.(훈련 데이터)
X = np.array(x)
Y = np.array(y)


# 다항식 회귀분석을 진행하기 위해 변수들을 조합합니다.
X_poly = []
for x_i in X:
    X_poly.append([
        x_i[0] ** 2, # X_1^2
        x_i[1], # X_2
        x_i[1] * x_i[2], # X_2 * X_3
        x_i[2] # X_3
    ])

# X, Y를 80:20으로 나눕니다. 80%는 트레이닝 데이터, 20%는 테스트 데이터입니다.
x_train, x_test, y_train, y_test = train_test_split(X_poly, Y, test_size=0.2, random_state=0)

# x_train, y_train에 대해 다항식 회귀분석을 진행합니다.
lrmodel = LinearRegression()
lrmodel.fit(x_train, y_train)

#x_train에 대해, 만든 회귀모델의 예측값을 구하고, 이 값과 y_train 의 차이를 이용해 MSE를 구합니다.
predicted_y_train = lrmodel.predict(x_train)
mse_train = mean_squared_error(y_train, predicted_y_train)
print("MSE on train data: {}".format(mse_train))

# x_test에 대해, 만든 회귀모델의 예측값을 구하고, 이 값과 y_test 의 차이를 이용해 MSE를 구합니다. 이 값이 1 미만이 되도록 모델을 구성해 봅니다.
predicted_y_test = lrmodel.predict(x_test)
mse_test = mean_squared_error(y_test, predicted_y_test)
print("MSE on test data: {}".format(mse_test))
```