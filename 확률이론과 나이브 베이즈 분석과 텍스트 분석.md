# 확률 기초(Introduction to Probability)

> 확률이란 : 어떤 사건이 일어날 것인지 혹은 일어났는지에 대한 지식 혹은 믿음을 표현하는 방법

## Early Probability Theory

A와 B가 주사위 게임을 해서 먼저 6번을 이긴 사람이 80만원을 차지하기로 했다.   
A가 5번, B가 3번 이긴 후 게임이 중단되었다면 판돈을 어떻게 나눠야 하는가?   
- “게임이 중단되기 전까지의 성적에 따라 나누자”
    - 판돈과 승리 판수의 비율로만 따지면 A는 50만원, B는 30만원 : 비합리적이다. A는 한번만 더 이기면 판돈을 모두 가져갈 수 있기 때문이다.
- 현재 상황: 6번 이기면 판돈을 모두 가져감 : A는 5번, B는 3번 승리

```
            9번째 판 10번째 판 11번째 판
A가 승리    O
B가 승리    X

9번째 판에서 A가 이길 확률 : 1/2(A의 승리)
10번째 판에서 A가 이길 확률 : 1/2(B의 승리) * 1/2(A의 승리) = 1/4
11번째 판에서 A가 이길 확률 : 1/2(B의 승리) * 1/2(B의 승리) * 1/2(A의 승리) = 1/8
따라서 A가 승리할 확률 = 1/2 + 1/4 + 1/8 = 7/8
B가 승리할 확률 = 1/8
따라서 판돈을 7:1로 나눠야 공평하다. A가 70만원, B가 10만원
```

### 확률 예제

- 강아지를 기르는 학생일 확률   
```
엘리스를 수강한 학생 1,000명에게 질문   
“강아지를 기르시나요?” : 예/아니오   

사건 Yes : 강아지를 기른다 -> 88명   
사건 No : 강아지를 기르지 않는다 -> 912명   

P(Yes) = 88/1000 = 0.088   
P(No) = 912/1000 = 0.912   
```

- 완벽한 주사위를 한 번 굴린다.   
```
짝수가 나오는 사건 A: {2, 4, 6}
홀수가 나오는 사건 B: {1, 3, 5}

P(A) = 0.5
P(B) = 0.5


합집합 P(A⋃B) = 1
교집합 P(A⋂B) = 0
```

```
짝수가 나오는 사건 A: {2, 4, 6}
3보다 큰 수가 나오는 사건 B: {4, 5, 6}

P(A) = 3/6 = 1/2
P(B) = 3/6 = 1/2

합집합 P(A⋃B) = 4/6 # {2, 4, 5, 6}
교집합 P(A⋂B) = 2/6 # {4, 6}
```

## 조건부확률 : 사건 B가 일어났을 때 A가 일어날 확률

`P(A|B) = P(A⋂B)/P(B)`   

- P(A|B) = 3보다 큰 수가 나왔을 때 그 수가 짝수일 확률 = P({4, 6}) / P({4, 5, 6}) = 2/3

- P(B|A) = 짝수가 나왔을 때 그 수가 3보다 클 확률 = P({4, 6}) / P({2, 4, 6}) = 2/3

### 확률로 파이 구해보기

π, 즉 원주율은 수학에서 가장 중요한 숫자 중 하나입니다. 원주율은 원 둘레와 지름의 비율로 3.141592653...입니다.

2차원 평면에 점을 무작위로 찍어서 원주율을 구해보겠습니다. 원리는 원의 넓이를 구하는 것입니다.

반지름이 1인 원이 있다고 가정해보겠습니다. 이 원은 높이, 너비가 22인 사각형에 들어갑니다. 이 때 원의 넓이는 다음과 같이 구할 수 있습니다.

πr^2 = π

즉, 반지름이 1인 원의 넓이는 π 입니다.

우리가 무작위로 찍은 점 중 몇 %가 원 안에 들어가 있을지를 알게 된다면, 그 원의 넓이를 추측해 낼 수 있습니다. 예로, 75%의 점이 원 안에 있다면 그 원의 넓이는 0.75*4 = 3.00.75×4=3.0 이 됩니다. 원의 넓이가 π 라고 했으므로, π 는 3이라고 추측할 수 있습니다.

```
import matplotlib as mpl
mpl.use("Agg")
import matplotlib.pyplot as plt
import numpy as np

import elice_utils

def main():
    plt.figure(figsize=(5,5))
    
    X = []
    Y = []
    
    # N을 10배씩 증가할 때 파이 값이 어떻게 변경되는지 확인해보세요.
    # 점점 3.14에 가까워집니다.
    N = 1000000
    
    for i in range(N): # N개의 랜덤한 X,Y값을 생성한다.
        # [0,1]까지의 값을 [0,2]까지의 값으로 바꾸고 -1을 하여 [-1,1]까지의 값으로 바꾸는 코드
        X.append(np.random.rand() * 2 - 1) 
        Y.append(np.random.rand() * 2 - 1)
    X = np.array(X)
    Y = np.array(Y)
    
    # norm, np.norm을 써도되고, 직접 구해도 된다. X*X = X ** 2
    distance_from_zero = np.sqrt(X * X + Y * Y) # np.sqrt() : 인자에 root 씌우기
    is_inside_circle = distance_from_zero <= 1
    
    print("Estimated pi = %f" % (np.average(is_inside_circle) * 4))
    
    plt.scatter(X, Y, c=is_inside_circle)
    plt.savefig('circle.png')
    elice_utils.send_image('circle.png')

if __name__ == "__main__":
    main()
```

========

# 베이즈 법칙(Bayes’ Rule)


- 빈도 주의자(Frequentist) vs 베이즈 주의자(Bayesian)

> “동전 하나를 던졌을 때 앞면이 나올 확률은 50%이다.”

- 빈도 주의자 : 이 동전을 수천, 수만 번 던졌을 때 그 중 앞면이 50%, 뒷면이 50% 나온다.
    - 사건의 수가 무한해지면 사건의 확률을 정확히 정의할 수 있다.

- 베이즈 주의자: 동전 던지기의 결과가 앞면이 나올 것이라는 확신 (혹은 믿음) 이 50%이다.
    - 사전지식을 확률에 반영할 수 있다 -> 확률은 확신 혹은 믿음으로 수렴할 수 있다.

- 베이즈 법칙   
`P(A|X) = P(X|A)P(A) / P(X)`   
상대적으로 알기 쉬운 확률을 이용해서 복잡한 확률을 구해낸다.   

- 베이즈 법칙: 유도하기   
```
1. P(A|X) = P(A⋂X)/P(X) # X가 일어날 확률 중 A와 X가 모두 일어날 확률
2. P(A|X)P(X) = P(A⋂X) = P(X⋂A) = P(X|A)P(A)
3. P(A|X)P(X) = P(X|A)P(A)
4. P(A|X) = P(X|A)P(A) / P(X)
```

- 베이즈 법칙을 알아서 어디에 사용할까?

## 예제 : 암 검사 키트

- 키트 검사 결과 양성 반응이 나왔다면, 암에 걸렸을 확률은?

```
암 A에 대한 테스트 키트가 있다.
임의의 사람이 이 암에 걸릴 확률은 1%이다.
즉, 전체 인구 중 암에 걸린 사람은 1%이다.
이 암을 진단할 수 있는 키트가 있는데,
암에 걸린 사람은 99%의 확률로 양성 반응이 나오고,
걸리지 않은 사람은 1%의 확률로 양성 반응이 나온다.
```

1. 암에 걸린 사건 : A
2. 키트에서 양성반응이 나온 사건 : X
3. P(A|X) : 키트에서 양성반응이 나왔을 때 암에 실제로 걸렸을 확률
4. P(X) : 키트에서 양성반응이 나올 확률 = 0.01
5. P(A) : 임의의 사람이 암에 걸렸을 확률 = 0.01
6. P(X|A) : 암에 걸렸을 때 키트에서 양성반응이 나올 확률 = 0.99
7. P(X) = P(X⋂A) + P(X⋂ㄱA) = P(X|A)P(A) + P(X|ㄱA)P(ㄱA) = 0.99*0.01 + 0.01*0.99 = 0.0198 = 키트에서 양성반응이 나올 확률
    - ㄱ : NOT
    - P(X⋂A) = P(X|A)P(A)
    - P(X⋂ㄱA) = P(X|ㄱA)P(ㄱA)
8. `P(A|X) = P(X|A)P(A) / P(X)` =  0.99*0.01/0.0198 = 0.5
9. 따라서 키트에서 양성반응이 나왔는데 내가 암에 걸렸을 확률은 50%이다.

### 실습: 유방암 검사 키트

40대 여성이 mammogram(X-ray) 검사를 통해 유방암 양성 의심 판정을 받았을 때 유방암을 실제로 가지고 있을 확률은 어떻게 될까요?

mammogram_test() 함수를 구현하며 베이즈 법칙을 직접 응용해보겠습니다. mammogram_test() 함수는 세 가지 숫자를 입력 받습니다.

- sensitivity - 검사의 민감성을 뜻합니다. 유방암 보유자를 대상으로 검사 결과가 양성으로 표시될 확률입니다. 0부터 1 사이의 값을 갖습니다.
- prior_prob - 총 인구를 기준으로 유방암을 가지고 있을 사전 확률(prior probability)입니다. 0.004 정도로 매우 낮은 값입니다.
- false_alarm - 실제로는 암을 갖고 있지 않지만 유방암이라고 진단될 확률입니다. 0.1 정도로 생각보다 높은 값입니다.

입력 받은 세 값을 바탕으로 나이브 법칙을 이용해 유방암 보유 여부를 확률로 출력합니다.

```
>>> 0.8
>>> 0.004
>>> 0.1
3.11%
```

- A=1 은 Mammogram 검사를 통해 암으로 진단되는 경우, B=1 은 실제로 유방암을 가지고 있는 경우입니다.

1. sensitivity는 P(A=1|B=1)로 표현할 수 있습니다. 암을 실제로 가지고 있을 때 암으로 진단될 확률이 80%라면
    `P(A=1|B=1) = 0.8`

2. 일반적으로 유방암을 가지고 있을 확률은, 즉 prior_prob의 값은 매우 낮습니다:
    `P(B=1) = 0.004`

3. 유방암을 가지고 있지 않을 확률은 1에서 prior_prob를 빼면 됩니다:
    `P(B=0) = 1 - P(B=1) = 0.996`

4. 실제로 암을 가지고 있지 않지만 암으로 진단되는 확률, false_alarm는 생각보다 매우 높습니다:
    `P(A=1|B=0) = 0.1`

5. Mammogram 검사를 통해 암으로 진단되는 경우의 확률, P(A=1)를 구해보겠습니다:

```
P(A=1)
= P(A=1|B=0)P(B=0) + P(A=1|B=1)P(B=1)
= 0.1*0.996 + 0.8*0.004
= 0.1028
```

6. 유방암 진단을 받았을 때 실제로 유방암을 가지고 있을 확률을 베이즈 법칙을 이용해 계산하면 다음과 같습니다:

```
P(B=1|A=1)
= P(A=1|B=1)P(B=1) / P(A=1)
= 0.8*0.004 / 0.1028
= ∼ 0.0311
```

```
def main():
    # 검사의 민감성을 뜻합니다. 유방암 보유자를 대상으로 검사 결과가 양성으로 표시될 확률입니다. 0부터 1 사이의 값을 갖습니다.
    sensitivity = 0.8
    # 총 인구를 기준으로 유방암을 가지고 있을 사전 확률(prior probability)입니다. 0.004 정도로 매우 낮은 값입니다.
    prior_prob = 0.004
    # 실제로는 암을 갖고 있지 않지만 유방암이라고 진단될 확률입니다. 0.1 정도로 생각보다 높은 값입니다.
    false_alarm = 0.1

    print("%.2lf%%" % (100 * mammogram_test(sensitivity, prior_prob, false_alarm)))

def mammogram_test(sensitivity, prior_prob, false_alarm):
    p_a1_b1 = sensitivity # p(A = 1 | B = 1)

    p_b1 = prior_prob    # p(B = 1)

    p_b0 = 1 - p_b1    # p(B = 0)

    p_a1_b0 = false_alarm # p(A = 1|B = 0)
    
    # P(A = 1) = P(A = 1 | B = 0)P(B = 0) + P(A = 1 | B = 1)P(B = 1)
    p_a1 = p_a1_b0 * p_b0 + p_a1_b1 * p_b1 # p(A = 1)

    p_b1_a1 = p_a1_b1 * p_b1 / p_a1 # p(B = 1|A = 1)

    return p_b1_a1

if __name__ == "__main__":
    main()
```

==============

# Naive Bayes 분류기

> 기계학습 - 지도학습 - 분류 : 주어진 데이터가 어떤 클래스에 속하는지 알아내는 작업

## 분류기(Classfication)

> 주어진 데이터가 어떤 클래스에 속하는지 알아내는 방법을 자동으로 학습하는 알고리즘

- input 데이터는 모두 레이블링 되어있다.
- 주어진 데이터를 잘 분류하는 함수(선)을 찾는 것이 목표지만, input 데이터를 정확하게 맞추는 복잡한 선(과대적합)은 오히려 일반적 문제에 대응하지 못한다.

### 사탕 기계

사탕 기계 A, B가 있다.    
이 둘은 같은 종류의 사탕을 내놓지만, 들어 있는 사탕의 비율이 다르다.   
```
비율    빨강색  노랑색  초록색
A       2       2       1
B       1       1       1
```

- 문제: 사탕 10개를 뽑아서 빨강색 4개, 노랑색 5개, 초록색 1개를 뽑았다면 이 사탕은 어느 기계에서 뽑은 것일까?
- 수학적 정의
    - X : 사탕 10개를 뽑아 그 결과를 관측한 사건
    - A : 사탕 기계 A에서 사탕을 뽑은 사건
    - B : 사탕 기계 B에서 사탕을 뽑은 사건

- P(A|X) 와 P(B|X) 를 비교하면 어떤 것이 더 클까?
    - P(A|X) = P(X|A)P(A)/P(X)
    - P(B|X) = P(X|B)P(B)/P(X)
    - P(A|X) : P(B|X) = P(X|A)P(A)/P(X) : P(X|B)P(B)/P(X) = P(X|A)P(A) : P(X|B)P(B)

- 사전 확률
    - A 기계보다 B 기계가 조금 더 좋은 자리에 있어서 일반적으로 더 많이 팔린다.
    - P(A) = 0.4, P(B) = 0.6

- Likelihood (우도) : 테스트하고 싶은 모델이 데이터를 얼마나 잘 만들어내는가
    - P(X|A) 부터 계산해 보자 = A라는 기계가 X라는 데이터를 얼마나 잘 뽑아내는가(확률)
    - P(A)는 사전확률로 주어졌다.
    - 가정 : 사탕 기계가 매우 커서, 그 안에 있는 사탕 수의 비율은 몇 개를 꺼내도 일정하게 유지된다
        - A에서 빨강 사탕을 꺼낼 확률 = 2/5
        - A에서 노랑 사탕을 꺼낼 확률 = 2/5
        - A에서 초록 사탕을 꺼낼 확률 = 1/5
    - A에서 빨강 사탕 4개를 꺼낼 확률 = (2/5)^4
    - A에서 노랑 사탕 5개를 꺼낼 확률 = (2/5)^5
    - A에서 초록 사탕 1개를 꺼낼 확률 = (1/5)^1
    - A에서 빨강 사탕 4개, 노랑 사탕 5개, 초록 사탕 1개를 꺼낼 확률 = (2/5)^4 * (2/5)^5 * (1/5)^1
    - 따라서 P(X|A)=(2/5)^4 * (2/5)^5 * (1/5)^1
    - P(X|B)=(1/3)^4 * (1/3)^5 * (1/3)^1
    - 순서도 고려해야한다. 따라서
        - P(X|A)=(2/5)^4 * (2/5)^5 * (1/5)^1 * C # 모든 경우의 수 C
        - P(X|B)=(1/3)^4 * (1/3)^5 * (1/3)^1 * C
        - P(A|X) : P(B|X) = P(A)((2/5)^4 * (2/5)^5 * (1/5)^1) : P(B)((1/3)^4 * (1/3)^5 * (1/3)^1) 
            = **5.243 * 10^-5 * 0.4 : 1.694 * 10^-5 * 0.6** = 0.674 : 0.326
        - 꺼낸 10개의 사탕은 A에서 나왔을 확률이 B에서 나왔을 확률보다 두 배 더 높다.

- 정리
    - P(A|X), P(B|X) : 사후확률(Posterior)
    - P(X|A), P(X|B) : Likelihood(우도)
    - P(A), P(B) : 사전확률(prior)

### 사탕가게 이론 코드로 구현해보기

```
import re
import math
import numpy as np

def main():
    M1 = {'r': 0.7, 'g': 0.2, 'b': 0.1} # M1 기계의 사탕 비율
    M2 = {'r': 0.3, 'g': 0.4, 'b': 0.3} # M2 기계의 사탕 비율
    
    # 다음과 같이 10개의 사탕을 뽑았을 때 이 사탕들이 몇 번째 기계에서 나왔을지 확률로 표현해보자
    test = {'r': 4, 'g': 3, 'b': 3}
    
    # 두 기계에서 사탕을 뽑을 확률은 다음과 같습니다.(사전확률 정의)
    print(naive_bayes(M1, M2, test, 0.7, 0.3))

def naive_bayes(M1, M2, test, M1_prior, M2_prior):
    M1_likelihood = M1['r'] ** test['r'] * M1['g'] ** test['g'] * M1['b'] ** test['b']
    M1_posterior = M1_likelihood * M1_prior
    
    M2_likelihood = M2['r'] ** test['r'] * M2['g'] ** test['g'] * M2['b'] ** test['b']
    M2_posterior = M2_likelihood * M2_prior
    
    # 두 복잡한 수를 더해서 1이 되도록 Nomalize
    M1_normalized = M1_posterior / (M1_posterior + M2_posterior)
    M2_normalized = M2_posterior / (M1_posterior + M2_posterior)
    
    return [M1_normalized, M2_normalized]

if __name__ == "__main__":
    main()
```

========

# Bag of Words 와 감정분석

## 텍스트 데이터의 분류

- 목표 : 감정분석 알고리즘을 만들어 본다
- 텍스트 데이터의 분류   
```
비율    좋아   최고    싫어    별로
긍정    3      5       1       1
부정    1      1       2       4
```

- 자연어 처리(Natural Languege Processing)를 어떻게 처리해서 분석기법을 적용할 것인가?
    - 오늘 나는 밥을 먹었다. 어제 나는 햄버거를 먹었다.
    - (1) 특수 문자 제거
        - 오늘 나는 밥을 먹었다 어제 나는 햄버거를 먹었다
    - (2) Tokenize
        - 오늘 / 나는 / 밥을 / 먹었다 / 어제 / 나는 / 햄버거를 / 먹었다
    - 한국어는 형태소 분석도 병행한다.
    - 섞어서 순서가 사리지도록 하고, 어떤 단어가 몇 개 있는지 정보만 남긴다
        - Python Dictionary로 표현하면 쉽다.
        - {"오늘": 1, "나는": 2, "먹었다": 2, "햄버거를": 1, "밥을": 1, "어제": 1}
    - BOW 모델에서 순서는 중요하지 않다.

### Bag of Words 실습

```
import re

special_chars_remover = re.compile("[^\w'|_]")

def main():
    sentence = "Bag-of-Words 모델을 Python으로 직접 구현하겠습니다. 주어진 문장에 대해 Bag-of-Words 모델과 단어 사진을 만든 뒤, 단어 사전과 만들어진 Bag-of-Words를 같이 리턴해야 합니다."
    bow = create_BOW(sentence)

    print(bow)

# create_BOW()는 문장 한 줄을 입력 받고 해당 문장의 단어 사전과 Bag of Words 모델을 순서대로 리턴합니다.
def create_BOW(sentence):
    bow = {}
    
    # 단어는 모두 소문자로 치환되어야 합니다. .lower()을 사용하세요.
    sentence_lowered = sentence.lower()
    
    # # 특수문자를 모두 제거합니다.
    sentence_without_special_characters = remove_special_characters(sentence_lowered)
    
    # 단어는 space 를 기준으로 잘라내어 만듭니다. split()을 사용하세요.
    splitted_sentence = sentence_without_special_characters.split()
    
    # 단어는 한 글자 이상이어야 합니다. 단어의 길이를 체크하기 위해 len()을 사용하세요. 빈 토큰 제거
    splitted_sentence_filtered = [
        token
        for token in splitted_sentence
        if len(token) >= 1
    ]
    
    # 사전처리된 단어를 bag of words에 넣습니다
    for token in splitted_sentence_filtered :
        bow.setdefault(token, 0) # 토큰이 없으면 0으로 리셋해라
        bow[token] += 1 # 그리고 1을 더하라, 있으면 0을 무시하고 점점 수가 올라간다.
        
        # if token not in bow :
        #     bow[token] = 1
        # else :
        #     bow[token] += 1
            
            
    return bow

# 특수문자를 모두 제거합니다. 정규식표현(re)이 사용된 remove_special_characters() 함수를 이용하세요.
def remove_special_characters(sentence):
    return special_chars_remover.sub(' ', sentence)


if __name__ == "__main__":
    main()
```

========

## 감정 분류기(Sentiment Classifier)

- 사탕 기계와 Bag of Words    
A : {"초록색": 1, "노랑색": 2, "빨강색": 2}
B : {"빨강색": 1, "초록색": 1, "노랑색": 1}

A 기계에서 노랑색 사탕이 나올 확률은? 2/5   
노랑색 사탕의 개수 / 초록 + 노랑 + 빨강 사탕의 개수 = 노랑색 사탕의 개수 / 전체 사탕의 개수   

- Likelihood 다시 보기   
A에서 빨강 사탕 4개, 노랑 사탕 5개, 초록 사탕 1개를 꺼낼 확률 = (2/5)^4 * (2/5)^5 * (1/5)^1   
순서는 고려하지 않는다. A, B의 비율만 알고싶기 때문에.

- Naive Bayes
P(A|X) : P(B|X) = P(X|A)P(A) : P(X|B)P(B) = = 5.243 * 10^5 * 0.4 : 1.694 * 10^5 * 0.6 = 0.674 : 0.326   

- Sentiment Classifier
긍정 : {"좋아": 3, "최고": 5, "싫어": 1, "별로": 1}    
부정 : {"좋아": 1, "최고": 1, "싫어": 2, "별로": 4}   

“긍정” 기계에서 “최고” 단어가 나올 확률은? = “최고” 단어의 개수 / 전체 단어의 개수 = 5/10   
“부정” 기계에서 “싫어” 단어가 나올 확률은? = “싫어” 단어의 개수 / 전체 단어의 개수 = 2/8   

### 학습 : Training

- 기계학습 알고리즘은 자동으로 학습되게 한다.

- 긍정적인 문서 2000개 부정적인 문서 2000개(지도학습 - 레이블이 붙은 input data)
    - 각각의 문서 셋들에서 나오는 단어의 빈도 수를 측정

- 트레이닝 결과
    - 부정 : [('영화', 11452), ('너무', 5417), ('진짜', 4698), ('정말', 3836), ('그냥', 3537), ('이', 3279), ('왜', 3108), ('이런', 2659), ('이건', 2253), ('좀', 2203), ('다', 2132), ('더', 2036), ('영화는', 2013), ('쓰레기', 1991),...   
        - 전체 단어 수 : 796,466
        - P(“쓰레기”|부정) = 0.00249
    - 긍정 : [('영화', 13930), ('정말', 7544), ('너무', 6046), ('진짜', 4325), ('이', 3988), ('최고의', 2776), ('수', 2695), ('잘', 2630), ('더', 2436), ('보고', 2178), ('최고', 2035), ('영화를', 2005), ('ㅋㅋ', 1998), ('그', 1960),...   
        - 전체 단어 수 : 773,502
        -  P(“쓰레기”|긍정) = 0.000111

- X : 마음이 따듯해지는 최고의 영화   
```
부정                                긍정
P(“마음이”|부정) = 0.000498         P(“마음이”|긍정) = 0.000228
P(“따뜻해지는”|부정) = 0.0000142    P(“따뜻해지는”|긍정) = 0.00135
P(“최고의”|부정) = 0.000111         P(“최고의”|긍정) = 0.00249
P(“영화”|부정) = 0.0180             P(“영화”|긍정) = 0.0143
1.420 * 10^-14           :            1.114 * 10^-11 
0.001                   :            0.999
```

- 단어가 없으면 어떡할까? X : 이거 볼 시간에 엘리스에서 공부하자
```
부정                                긍정
P(“시간에”|부정) = 0.0000313        P(“시간에”|긍정) = 0.0000459
P(“엘리스”|부정) = 0                P(“엘리스”|긍정) = 0 => 10^-10처럼 아주 작은 상수를 대신 넣어주면 된다.
P(“에서”|부정) = 0.00184            P(“에서”|긍정) = 0.000838
P(“공부하자”|부정) = 0.00484        P(“공부하자”|긍정) = 0.00112
0                       :           0
0                       :           0
```

```
부정                                긍정
P(“시간에”|부정) = 0.0000313        P(“시간에”|긍정) = 0.0000459
P(“엘리스”|부정) = 0.00000001       P(“엘리스”|긍정) = 0.00000001
P(“에서”|부정) = 0.00184            P(“에서”|긍정) = 0.000838
P(“공부하자”|부정) = 0.00484        P(“공부하자”|긍정) = 0.00112
2.787 * 10^-18               :       4.308 * 10^-19
0.866                        :       0.133
```